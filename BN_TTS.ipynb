{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91bebc2b-d14e-473e-be1e-37987caf0246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hparams.py\n",
    "\n",
    "\"\"\"Hyper parameters.\"\"\"\n",
    "__author__ = 'Erdene-Ochir Tuguldur'\n",
    "\n",
    "\n",
    "class HParams:\n",
    "    \"\"\"Hyper parameters\"\"\"\n",
    "\n",
    "    disable_progress_bar = False  # set True if you don't want the progress bar in the console\n",
    "\n",
    "    logdir = \"logdir\"  # log dir where the checkpoints and tensorboard files are saved\n",
    "\n",
    "    # audio.py options, these values are from https://github.com/Kyubyong/dc_tts/blob/master/hyperparams.py\n",
    "    reduction_rate = 4  # melspectrogram reduction rate, don't change because SSRN is using this rate\n",
    "    n_fft = 2048 # fft points (samples)\n",
    "    n_mels = 80  # Number of Mel banks to generate\n",
    "    power = 1.5  # Exponent for amplifying the predicted magnitude\n",
    "    n_iter = 50  # Number of inversion iterations\n",
    "    preemphasis = .97\n",
    "    max_db = 100\n",
    "    ref_db = 20\n",
    "    sr = 22050  # Sampling rate\n",
    "    frame_shift = 0.0125  # seconds\n",
    "    frame_length = 0.05  # seconds\n",
    "    hop_length = int(sr * frame_shift)  # samples. =276.\n",
    "    win_length = int(sr * frame_length)  # samples. =1102.\n",
    "    max_N = 180  # Maximum number of characters.\n",
    "    max_T = 210  # Maximum number of mel frames.\n",
    "\n",
    "    e = 128  # embedding dimension\n",
    "    d = 256  # Text2Mel hidden unit dimension\n",
    "    c = 512+128  # SSRN hidden unit dimension\n",
    "\n",
    "    dropout_rate = 0.05  # dropout\n",
    "\n",
    "    # Text2Mel network options\n",
    "    text2mel_lr = 0.005  # learning rate\n",
    "    text2mel_max_iteration = 3000  # max train step\n",
    "    text2mel_weight_init = 'none'  # 'kaiming', 'xavier' or 'none'\n",
    "    text2mel_normalization = 'layer'  # 'layer', 'weight' or 'none'\n",
    "    text2mel_basic_block = 'gated_conv'  # 'highway', 'gated_conv' or 'residual'\n",
    "\n",
    "    # SSRN network options\n",
    "    ssrn_lr = 0.0005  # learning rate\n",
    "    ssrn_max_iteration = 1500  # max train step\n",
    "    ssrn_weight_init = 'kaiming'  # 'kaiming', 'xavier' or 'none'\n",
    "    ssrn_normalization = 'weight'  # 'layer', 'weight' or 'none'\n",
    "    ssrn_basic_block = 'residual'  # 'highway', 'gated_conv' or 'residual'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7deabbc-a4fe-40b7-b2e8-2c2af6a6b326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#audio.py\n",
    "\n",
    "\"\"\"These methods are copied from https://github.com/Kyubyong/dc_tts/\"\"\"\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import librosa\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "hp = HParams()\n",
    "\n",
    "def spectrogram2wav(mag):\n",
    "    '''# Generate wave file from linear magnitude spectrogram\n",
    "    Args:\n",
    "      mag: A numpy array of (T, 1+n_fft//2)\n",
    "    Returns:\n",
    "      wav: A 1-D numpy array.\n",
    "    '''\n",
    "    # transpose\n",
    "    mag = mag.T\n",
    "\n",
    "    # de-noramlize\n",
    "    mag = (np.clip(mag, 0, 1) * hp.max_db) - hp.max_db + hp.ref_db\n",
    "\n",
    "    # to amplitude\n",
    "    mag = np.power(10.0, mag * 0.05)\n",
    "\n",
    "    # wav reconstruction\n",
    "    wav = griffin_lim(mag ** hp.power)\n",
    "\n",
    "    # de-preemphasis\n",
    "    wav = signal.lfilter([1], [1, -hp.preemphasis], wav)\n",
    "\n",
    "    # trim\n",
    "    wav, _ = librosa.effects.trim(wav)\n",
    "\n",
    "    return wav.astype(np.float32)\n",
    "\n",
    "\n",
    "def griffin_lim(spectrogram):\n",
    "    '''Applies Griffin-Lim's raw.'''\n",
    "    X_best = copy.deepcopy(spectrogram)\n",
    "    for i in range(hp.n_iter):\n",
    "        X_t = invert_spectrogram(X_best)\n",
    "        est = librosa.stft(X_t, hp.n_fft, hp.hop_length, win_length=hp.win_length)\n",
    "        phase = est / np.maximum(1e-8, np.abs(est))\n",
    "        X_best = spectrogram * phase\n",
    "    X_t = invert_spectrogram(X_best)\n",
    "    y = np.real(X_t)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def invert_spectrogram(spectrogram):\n",
    "    '''Applies inverse fft.\n",
    "    Args:\n",
    "      spectrogram: [1+n_fft//2, t]\n",
    "    '''\n",
    "    return librosa.istft(spectrogram, hp.hop_length, win_length=hp.win_length, window=\"hann\")\n",
    "\n",
    "\n",
    "def get_spectrograms(fpath):\n",
    "    '''Parse the wave file in `fpath` and\n",
    "    Returns normalized melspectrogram and linear spectrogram.\n",
    "    Args:\n",
    "      fpath: A string. The full path of a sound file.\n",
    "    Returns:\n",
    "      mel: A 2d array of shape (T, n_mels) and dtype of float32.\n",
    "      mag: A 2d array of shape (T, 1+n_fft/2) and dtype of float32.\n",
    "    '''\n",
    "    # Loading sound file\n",
    "    y, sr = librosa.load(fpath, sr=hp.sr)\n",
    "\n",
    "    # Trimming\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    # Preemphasis\n",
    "    y = np.append(y[0], y[1:] - hp.preemphasis * y[:-1])\n",
    "\n",
    "    # stft\n",
    "    linear = librosa.stft(y=y,\n",
    "                          n_fft=hp.n_fft,\n",
    "                          hop_length=hp.hop_length,\n",
    "                          win_length=hp.win_length)\n",
    "\n",
    "    # magnitude spectrogram\n",
    "    mag = np.abs(linear)  # (1+n_fft//2, T)\n",
    "\n",
    "    # mel spectrogram\n",
    "    #mel_basis = librosa.filters.mel(hp.sr, hp.n_fft, hp.n_mels)  # (n_mels, 1+n_fft//2)\n",
    "    mel_basis = librosa.filters.mel(sr=22050, n_fft=2048, n_mels=128)\n",
    "    mel = np.dot(mel_basis, mag)  # (n_mels, t)\n",
    "\n",
    "    # to decibel\n",
    "    mel = 20 * np.log10(np.maximum(1e-5, mel))\n",
    "    mag = 20 * np.log10(np.maximum(1e-5, mag))\n",
    "\n",
    "    # normalize\n",
    "    mel = np.clip((mel - hp.ref_db + hp.max_db) / hp.max_db, 1e-8, 1)\n",
    "    mag = np.clip((mag - hp.ref_db + hp.max_db) / hp.max_db, 1e-8, 1)\n",
    "\n",
    "    # Transpose\n",
    "    mel = mel.T.astype(np.float32)  # (T, n_mels)\n",
    "    mag = mag.T.astype(np.float32)  # (T, 1+n_fft//2)\n",
    "\n",
    "    return mel, mag\n",
    "\n",
    "\n",
    "def save_to_wav(mag, filename):\n",
    "    \"\"\"Generate and save an audio file from the given linear spectrogram using Griffin-Lim.\"\"\"\n",
    "    wav = spectrogram2wav(mag)\n",
    "    scipy.io.wavfile.write(filename, hp.sr, wav)\n",
    "\n",
    "\n",
    "def preprocess(dataset_path, speech_dataset):\n",
    "    \"\"\"Preprocess the given dataset.\"\"\"\n",
    "    wavs_path = os.path.join(dataset_path, 'wavs')\n",
    "    mels_path = os.path.join(dataset_path, 'mels')\n",
    "    if not os.path.isdir(mels_path):\n",
    "        os.mkdir(mels_path)\n",
    "    mags_path = os.path.join(dataset_path, 'mags')\n",
    "    if not os.path.isdir(mags_path):\n",
    "        os.mkdir(mags_path)\n",
    "\n",
    "    for fname in tqdm(speech_dataset.fnames):\n",
    "        mel, mag = get_spectrograms(os.path.join(wavs_path, '%s.wav' % fname))\n",
    "\n",
    "        t = mel.shape[0]\n",
    "        # Marginal padding for reduction shape sync.\n",
    "        num_paddings = hp.reduction_rate - (t % hp.reduction_rate) if t % hp.reduction_rate != 0 else 0\n",
    "        mel = np.pad(mel, [[0, num_paddings], [0, 0]], mode=\"constant\")\n",
    "        mag = np.pad(mag, [[0, num_paddings], [0, 0]], mode=\"constant\")\n",
    "        # Reduction\n",
    "        mel = mel[::hp.reduction_rate, :]\n",
    "\n",
    "        np.save(os.path.join(mels_path, '%s.npy' % fname), mel)\n",
    "        np.save(os.path.join(mags_path, '%s.npy' % fname), mag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bcae901-8325-4ae4-ba83-2c120c1bf786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ljspeech.py\n",
    "\n",
    "\"\"\"Data loader for the LJSpeech dataset. See: https://keithito.com/LJ-Speech-Dataset/\"\"\"\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#vocab = \"PE abcdefghijklmnopqrstuvwxyz'.?\"  # P: Padding, E: EOS.\n",
    "vocab = \"PE অআইঈউঊঋএঐওঔা ি ী ু ূ ৃ ে ৈ ো ৌক খ গ ঘ ঙ চ ছ জ ঝ ঞ ট ঠ ড ঢ ণত থ দ ধ ন প ফ ব ভমযরলশষসহড়ঢ়য়ৎংঃঁ্ঽ‍্য‍  ‍্র'\"\n",
    "#vocab = \"PE অআইঈউঊঋএঐওঔা ি ী ু ূ ৃ ে ৈ ো ৌক খ গ ঘ ঙ চ ছ জ ঝ ঞ ট ঠ ড ঢ ণত থ দ ধ ন প ফ ব ভমযরলশষসহড়ঢ়য়ৎংঃঁ্ঽ‍্য‍  ‍্র'.?\"\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def text_normalize(text):\n",
    "    text = ''.join(char for char in unicodedata.normalize('NFD', text)\n",
    "                   if unicodedata.category(char) != 'Mn')  # Strip accents\n",
    "    #print(text)\n",
    "    #text = text.lower()\n",
    "    text = re.sub(\"[^{}]()\".format(vocab), \" \", text)\n",
    "    text = re.sub(\"[ ]+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def read_metadata(metadata_file):\n",
    "    fnames, text_lengths, texts = [], [], []\n",
    "    transcript = os.path.join(metadata_file)\n",
    "    #transcript = \"/content/drive/My Drive/TTS_B/datasets/LJSpeech-1.1/line_index.tsv\"\n",
    "    lines = codecs.open(transcript, 'r', 'utf-8').readlines()\n",
    "    for line in lines:\n",
    "        fname, text = line.strip().split(\"\\t\")\n",
    "\n",
    "        fnames.append(fname)\n",
    "\n",
    "        text = text_normalize(text) + \"E\"  # E: EOS\n",
    "        text = [char2idx[char] for char in text]\n",
    "        text_lengths.append(len(text))\n",
    "        texts.append(np.array(text, np.float32))\n",
    "\n",
    "    return fnames, text_lengths, texts\n",
    "\n",
    "\n",
    "def get_test_data(sentences, max_n):\n",
    "    normalized_sentences = [text_normalize(line).strip() + \"E\" for line in sentences]  # text normalization, E: EOS\n",
    "    texts = np.zeros((len(normalized_sentences), max_n + 1), np.float32)\n",
    "    for i, sent in enumerate(normalized_sentences):\n",
    "        texts[i, :len(sent)] = [char2idx[char] for char in sent]\n",
    "    return texts\n",
    "\n",
    "\n",
    "class LJSpeech(Dataset):\n",
    "    def __init__(self, keys, dir_name='bn_bd'):\n",
    "        self.keys = keys\n",
    "        self.path = os.path.join(\"/home/nipu/ml\", dir_name)\n",
    "        self.fnames, self.text_lengths, self.texts = read_metadata(os.path.join(self.path, \"line_index.tsv\"))\n",
    "\n",
    "    def slice(self, start, end):\n",
    "        self.fnames = self.fnames[start:end]\n",
    "        self.text_lengths = self.text_lengths[start:end]\n",
    "        self.texts = self.texts[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {}\n",
    "        if 'texts' in self.keys:\n",
    "            data['texts'] = self.texts[index]\n",
    "        if 'mels' in self.keys:\n",
    "            # (39, 80)\n",
    "            data['mels'] = np.load(os.path.join(self.path, 'mels', \"%s.npy\" % self.fnames[index]))\n",
    "        if 'mags' in self.keys:\n",
    "            # (39, 80)\n",
    "            data['mags'] = np.load(os.path.join(self.path, 'mags', \"%s.npy\" % self.fnames[index]))\n",
    "        if 'mel_gates' in self.keys:\n",
    "            data['mel_gates'] = np.ones(data['mels'].shape[0], dtype=np.int)  # TODO: because pre processing!\n",
    "        if 'mag_gates' in self.keys:\n",
    "            data['mag_gates'] = np.ones(data['mags'].shape[0], dtype=np.int)  # TODO: because pre processing!\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216d325-0ca9-4f05-b7bc-229e1f7ad082",
   "metadata": {},
   "source": [
    "he librosa.filters.mel() function creates a Mel filter-bank. This produces a linear transformation matrix to project FFT bins onto Mel-frequency bins.\n",
    "\n",
    "The Mel scale is a quasi-logarithmic function of acoustic frequency designed such that perceptually similar pitch intervals (e.g. octaves) appear equal in width over the full hearing range. This makes it a useful scale for representing and analyzing audio signals, as it more closely matches the way humans perceive sound.\n",
    "\n",
    "Mel filter-banks are commonly used in a variety of audio processing tasks, such as speech recognition, music information retrieval, and automatic speaker recognition.\n",
    "\n",
    "The librosa.filters.mel() function takes the following arguments:\n",
    "\n",
    "sr: The sampling rate of the incoming signal (in Hz).\n",
    "n_fft: The number of FFT components.\n",
    "n_mels: The number of Mel bands to generate.\n",
    "fmin: The lowest frequency (in Hz).\n",
    "fmax: The highest frequency (in Hz). If None, use fmax = sr / 2.0.\n",
    "htk: Whether to use the HTK formula instead of Slaney.\n",
    "norm: The type of normalization to apply to the filters. Can be None, slaney, or a number.\n",
    "dtype: The data type of the output basis.\n",
    "The function returns a NumPy array of shape (n_mels, 1 + n_fft / 2), which represents the Mel filter-bank.\n",
    "\n",
    "Here is an example of how to use the librosa.filters.mel() function:\n",
    "\n",
    "Python\n",
    "import librosa\n",
    "\n",
    "# Create a Mel filter-bank with 128 Mel bands\n",
    "melfb = librosa.filters.mel(sr=22050, n_fft=2048, n_mels=128)\n",
    "\n",
    "# Compute the Mel spectrogram of an audio signal\n",
    "audio_data, sr = librosa.load('audio.wav')\n",
    "mel_spectrogram = librosa.feature.melspectrogram(audio_data, sr=sr, S=melfb)\n",
    "Use code with caution. Learn more\n",
    "The mel_spectrogram variable will now contain a NumPy array of shape (n_mels, n_frames), which represents the Mel spectrogram of the audio signal.\n",
    "\n",
    "Mel filter-banks are a powerful tool for audio processing, and the librosa.filters.mel() function makes it easy to create them in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9355dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b87b2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = \"PE অআইঈউঊঋএঐওঔা ি ী ু ূ ৃ ে ৈ ো ৌক খ গ ঘ ঙ চ ছ জ ঝ ঞ ট ঠ ড ঢ ণত থ দ ধ ন প ফ ব ভমযরলশষসহড়ঢ়য়ৎংঃঁ্ঽ‍্য‍  ‍্র'\"\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6acae5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_normalize(text):\n",
    "    text = ''.join(char for char in unicodedata.normalize('NFKC', text) # NFKC = \n",
    "                   if unicodedata.category(char) != 'Mn')  # Strip accents\n",
    "    # print(text)\n",
    "    text = re.sub(\"[^{}]()\".format(vocab), \" \", text)\n",
    "    text = re.sub(\"[ ()]+\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f01e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমার,++ নাম :(শামীম) ক্কা? কক্ষ মুখ্য পত্র, বন্ধু\n"
     ]
    }
   ],
   "source": [
    "ss = text_normalize(\"আমার,++ নাম :(শামীম) ক্কা? কক্ষ মুখ্য পত্র, বন্ধু\")\n",
    "print(unicodedata.normalize('NFKC', \"আমার,++ নাম :(শামীম) ক্কা? কক্ষ মুখ্য পত্র, বন্ধু\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8afa194e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ban_00737_00107291991\tকেয়া ডেভেলপারস দেশের বিভিন্ন স্থানে স্থাপনা তৈরি করে থাকে\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = codecs.open('bn_bd/line_index.tsv', 'r', 'utf-8').readlines()\n",
    "len(lines[1])\n",
    "line = lines[5]\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c90f6fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ban_00737_00107291991 \n",
      " কেয়া ডেভেলপারস দেশের বিভিন্ন স্থানে স্থাপনা তৈরি করে থাকে\n"
     ]
    }
   ],
   "source": [
    "label, text = line.strip().split(\"\\t\")\n",
    "print(label.strip(), \"\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f29a3ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TTS_Bn\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from os.path import exists, join, expanduser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "277acebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nipu/ml/bn_bd\n"
     ]
    }
   ],
   "source": [
    "datasets_path = '/home/nipu/ml'\n",
    "dataset_path = os.path.join(datasets_path, 'bn_bd')\n",
    "print(dataset_path)\n",
    "\n",
    "if os.path.isdir(dataset_path) and False:\n",
    "  print(\"BN dataset folder already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44cad30f-3fb4-42d5-834d-4c780756145a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1891/1891 [01:29<00:00, 21.05it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"pre processing...\")\n",
    "lj_speech = LJSpeech([])\n",
    "# print(lj_speech)\n",
    "# print(dataset_path, lj_speech)\n",
    "preprocess(dataset_path, lj_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "975a8a2d-5e80-4f1d-bf4e-293708a8f35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "__author__ = 'Erdene-Ochir Tuguldur'\n",
    "__all__ = ['E', 'D', 'C', 'HighwayBlock', 'GatedConvBlock', 'ResidualBlock']\n",
    "\n",
    "#layers\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from hparams import HParams as hp\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        \"\"\"Layer Norm.\"\"\"\n",
    "        super(LayerNorm, self).__init__(normalized_shape, eps=eps, elementwise_affine=elementwise_affine)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # PyTorch LayerNorm seems to be expect (B, T, C)\n",
    "        y = super(LayerNorm, self).forward(x)\n",
    "        y = y.permute(0, 2, 1)  # reverse\n",
    "        return y\n",
    "\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, weight_init='none', normalization='weight', nonlinearity='linear'):\n",
    "        \"\"\"1D Deconvolution.\"\"\"\n",
    "        super(D, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size,\n",
    "                                         stride=2,  # paper: stride of deconvolution is always 2\n",
    "                                         dilation=dilation)\n",
    "\n",
    "        if normalization == 'weight':\n",
    "            self.deconv = nn.utils.weight_norm(self.deconv)\n",
    "        elif normalization == 'layer':\n",
    "            self.layer_norm = LayerNorm(out_channels)\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        if weight_init == 'kaiming':\n",
    "            nn.init.kaiming_normal_(self.deconv.weight, mode='fan_out', nonlinearity=nonlinearity)\n",
    "        elif weight_init == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.deconv.weight, nn.init.calculate_gain(nonlinearity))\n",
    "\n",
    "    def forward(self, x, output_size=None):\n",
    "        y = self.deconv(x, output_size=output_size)\n",
    "        if hasattr(self, 'layer_norm'):\n",
    "            y = self.layer_norm(y)\n",
    "        y = F.dropout(y, p=hp.dropout_rate, training=self.training, inplace=True)\n",
    "        if self.nonlinearity == 'relu':\n",
    "            y = F.relu(y, inplace=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "class C(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, causal=False, weight_init='none', normalization='weight', nonlinearity='linear'):\n",
    "        \"\"\"1D convolution.\n",
    "        The argument 'causal' indicates whether the causal convolution should be used or not.\n",
    "        \"\"\"\n",
    "        super(C, self).__init__()\n",
    "        self.causal = causal\n",
    "        if causal:\n",
    "            self.padding = (kernel_size - 1) * dilation\n",
    "        else:\n",
    "            self.padding = (kernel_size - 1) * dilation // 2\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                              stride=1,  # paper: 'The stride of convolution is always 1.'\n",
    "                              padding=self.padding, dilation=dilation)\n",
    "\n",
    "        if normalization == 'weight':\n",
    "            self.conv = nn.utils.weight_norm(self.conv)\n",
    "        elif normalization == 'layer':\n",
    "            self.layer_norm = LayerNorm(out_channels)\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        if weight_init == 'kaiming':\n",
    "            nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity=nonlinearity)\n",
    "        elif weight_init == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.conv.weight, nn.init.calculate_gain(nonlinearity))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        padding = self.padding\n",
    "        if self.causal and padding > 0:\n",
    "            y = y[:, :, :-padding]\n",
    "\n",
    "        if hasattr(self, 'layer_norm'):\n",
    "            y = self.layer_norm(y)\n",
    "        y = F.dropout(y, p=hp.dropout_rate, training=self.training, inplace=True)\n",
    "        if self.nonlinearity == 'relu':\n",
    "            y = F.relu(y, inplace=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "class E(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(E, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class HighwayBlock(nn.Module):\n",
    "    def __init__(self, d, k, delta, causal=False, weight_init='none', normalization='weight'):\n",
    "        \"\"\"Highway Network like layer: https://arxiv.org/abs/1505.00387\n",
    "        The input and output shapes remain same.\n",
    "        Args:\n",
    "            d: input channel\n",
    "            k: kernel size\n",
    "            delta: dilation\n",
    "            causal: causal convolution or not\n",
    "        \"\"\"\n",
    "        super(HighwayBlock, self).__init__()\n",
    "        self.d = d\n",
    "        self.C = C(in_channels=d, out_channels=2 * d, kernel_size=k, dilation=delta, causal=causal, weight_init=weight_init, normalization=normalization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        L = self.C(x)\n",
    "        H1 = L[:, :self.d, :]\n",
    "        H2 = L[:, self.d:, :]\n",
    "        sigH1 = F.sigmoid(H1)\n",
    "        return sigH1 * H2 + (1 - sigH1) * x\n",
    "\n",
    "\n",
    "class GatedConvBlock(nn.Module):\n",
    "    def __init__(self, d, k, delta, causal=False, weight_init='none', normalization='weight'):\n",
    "        \"\"\"Gated convolutional layer: https://arxiv.org/abs/1612.08083\n",
    "        The input and output shapes remain same.\n",
    "        Args:\n",
    "            d: input channel\n",
    "            k: kernel size\n",
    "            delta: dilation\n",
    "            causal: causal convolution or not\n",
    "        \"\"\"\n",
    "        super(GatedConvBlock, self).__init__()\n",
    "        self.C = C(in_channels=d, out_channels=2 * d, kernel_size=k, dilation=delta, causal=causal,\n",
    "                   weight_init=weight_init, normalization=normalization)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        L = self.C(x)\n",
    "        return self.glu(L) + x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, d, k, delta, causal=False, weight_init='none', normalization='weight',\n",
    "                 widening_factor=2):\n",
    "        \"\"\"Residual block: https://arxiv.org/abs/1512.03385\n",
    "        The input and output shapes remain same.\n",
    "        Args:\n",
    "            d: input channel\n",
    "            k: kernel size\n",
    "            delta: dilation\n",
    "            causal: causal convolution or not\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.C1 = C(in_channels=d, out_channels=widening_factor * d, kernel_size=k, dilation=delta, causal=causal,\n",
    "                    weight_init=weight_init, normalization=normalization, nonlinearity='relu')\n",
    "        self.C2 = C(in_channels=widening_factor * d, out_channels=d, kernel_size=k, dilation=delta, causal=causal,\n",
    "                    weight_init=weight_init, normalization=normalization, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.C2(self.C1(x)) + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95f4f2ab-f507-4a99-9775-6bc767deccf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ssrn.py\n",
    "\n",
    "\"\"\"\n",
    "Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara\n",
    "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention\n",
    "https://arxiv.org/abs/1710.08969\n",
    "\n",
    "SSRN Network.\n",
    "\"\"\"\n",
    "__author__ = 'Erdene-Ochir Tuguldur'\n",
    "__all__ = ['SSRN']\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def Conv(in_channels, out_channels, kernel_size, dilation, nonlinearity='linear'):\n",
    "    return C(in_channels, out_channels, kernel_size, dilation, causal=False,\n",
    "             weight_init=hp.ssrn_weight_init, normalization=hp.ssrn_normalization, nonlinearity=nonlinearity)\n",
    "\n",
    "\n",
    "def DeConv(in_channels, out_channels, kernel_size, dilation, nonlinearity='linear'):\n",
    "    return D(in_channels, out_channels, kernel_size, dilation,\n",
    "             weight_init=hp.ssrn_weight_init, normalization=hp.ssrn_normalization, nonlinearity=nonlinearity)\n",
    "\n",
    "\n",
    "def BasicBlock(d, k, delta):\n",
    "    if hp.ssrn_basic_block == 'gated_conv':\n",
    "        return GatedConvBlock(d, k, delta, causal=False,\n",
    "                              weight_init=hp.ssrn_weight_init, normalization=hp.ssrn_normalization)\n",
    "    elif hp.ssrn_basic_block == 'highway':\n",
    "        return HighwayBlock(d, k, delta, causal=False,\n",
    "                            weight_init=hp.ssrn_weight_init, normalization=hp.ssrn_normalization)\n",
    "    else:\n",
    "        return ResidualBlock(d, k, delta, causal=False,\n",
    "                             weight_init=hp.ssrn_weight_init, normalization=hp.ssrn_normalization,\n",
    "                             widening_factor=1)\n",
    "\n",
    "\n",
    "class SSRN(nn.Module):\n",
    "    def __init__(self, c=hp.c, f=hp.n_mels, f_prime=(1 + hp.n_fft // 2)):\n",
    "        \"\"\"Spectrogram super-resolution network.\n",
    "        Args:\n",
    "            c: SSRN dim\n",
    "            f: Number of mel bins\n",
    "            f_prime: full spectrogram dim\n",
    "        Input:\n",
    "            Y: (B, f, T) predicted melspectrograms\n",
    "        Outputs:\n",
    "            Z_logit: logit of Z\n",
    "            Z: (B, f_prime, 4*T) full spectrograms\n",
    "        \"\"\"\n",
    "        super(SSRN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            Conv(f, c, 1, 1),\n",
    "\n",
    "            BasicBlock(c, 3, 1), BasicBlock(c, 3, 3),\n",
    "\n",
    "            DeConv(c, c, 2, 1), BasicBlock(c, 3, 1), BasicBlock(c, 3, 3),\n",
    "            DeConv(c, c, 2, 1), BasicBlock(c, 3, 1), BasicBlock(c, 3, 3),\n",
    "\n",
    "            Conv(c, 2 * c, 1, 1),\n",
    "\n",
    "            BasicBlock(2 * c, 3, 1), BasicBlock(2 * c, 3, 1),\n",
    "\n",
    "            Conv(2 * c, f_prime, 1, 1),\n",
    "\n",
    "            # Conv(f_prime, f_prime, 1, 1, nonlinearity='relu'),\n",
    "            # Conv(f_prime, f_prime, 1, 1, nonlinearity='relu'),\n",
    "            BasicBlock(f_prime, 1, 1),\n",
    "\n",
    "            Conv(f_prime, f_prime, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z_logit = self.layers(x)\n",
    "        Z = F.sigmoid(Z_logit)\n",
    "        return Z_logit, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f7311e9-8ee2-4864-af39-2574b4495f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#text2mel.py\n",
    "\n",
    "\"\"\"\n",
    "Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara\n",
    "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention\n",
    "https://arxiv.org/abs/1710.08969\n",
    "\n",
    "Text2Mel Network.\n",
    "\"\"\"\n",
    "__author__ = 'Erdene-Ochir Tuguldur'\n",
    "__all__ = ['Text2Mel']\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def Conv(in_channels, out_channels, kernel_size, dilation, causal=False, nonlinearity='linear'):\n",
    "    return C(in_channels, out_channels, kernel_size, dilation, causal=causal,\n",
    "             weight_init=hp.text2mel_weight_init, normalization=hp.text2mel_normalization, nonlinearity=nonlinearity)\n",
    "\n",
    "\n",
    "def BasicBlock(d, k, delta, causal=False):\n",
    "    if hp.text2mel_basic_block == 'gated_conv':\n",
    "        return GatedConvBlock(d, k, delta, causal=causal,\n",
    "                              weight_init=hp.text2mel_weight_init, normalization=hp.text2mel_normalization)\n",
    "    elif hp.text2mel_basic_block == 'highway':\n",
    "        return HighwayBlock(d, k, delta, causal=causal,\n",
    "                            weight_init=hp.text2mel_weight_init, normalization=hp.text2mel_normalization)\n",
    "    else:\n",
    "        return ResidualBlock(d, k, delta, causal=causal,\n",
    "                             weight_init=hp.text2mel_weight_init, normalization=hp.text2mel_normalization,\n",
    "                             widening_factor=2)\n",
    "\n",
    "\n",
    "def CausalConv(in_channels, out_channels, kernel_size, dilation, nonlinearity='linear'):\n",
    "    return Conv(in_channels, out_channels, kernel_size, dilation, causal=True, nonlinearity=nonlinearity)\n",
    "\n",
    "\n",
    "def CausalBasicBlock(d, k, delta):\n",
    "    return BasicBlock(d, k, delta, causal=True)\n",
    "\n",
    "\n",
    "class TextEnc(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, e=hp.e, d=hp.d):\n",
    "        \"\"\"Text encoder network.\n",
    "        Args:\n",
    "            vocab: vocabulary\n",
    "            e: embedding dim\n",
    "            d: Text2Mel dim\n",
    "        Input:\n",
    "            L: (B, N) text inputs\n",
    "        Outputs:\n",
    "            K: (B, d, N) keys\n",
    "            V: (N, d, N) values\n",
    "        \"\"\"\n",
    "        super(TextEnc, self).__init__()\n",
    "        self.d = d\n",
    "        self.embedding = E(len(vocab), e)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            Conv(e, 2 * d, 1, 1, nonlinearity='relu'),\n",
    "            Conv(2 * d, 2 * d, 1, 1),\n",
    "\n",
    "            BasicBlock(2 * d, 3, 1), BasicBlock(2 * d, 3, 3), BasicBlock(2 * d, 3, 9), BasicBlock(2 * d, 3, 27),\n",
    "            BasicBlock(2 * d, 3, 1), BasicBlock(2 * d, 3, 3), BasicBlock(2 * d, 3, 9), BasicBlock(2 * d, 3, 27),\n",
    "\n",
    "            BasicBlock(2 * d, 3, 1), BasicBlock(2 * d, 3, 1),\n",
    "\n",
    "            BasicBlock(2 * d, 1, 1), BasicBlock(2 * d, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = out.permute(0, 2, 1)  # change to (B, e, N)\n",
    "        out = self.layers(out)  # (B, 2*d, N)\n",
    "        K = out[:, :self.d, :]  # (B, d, N)\n",
    "        V = out[:, self.d:, :]  # (B, d, N)\n",
    "        return K, V\n",
    "\n",
    "\n",
    "class AudioEnc(nn.Module):\n",
    "    def __init__(self, d=hp.d, f=hp.n_mels):\n",
    "        \"\"\"Audio encoder network.\n",
    "        Args:\n",
    "            d: Text2Mel dim\n",
    "            f: Number of mel bins\n",
    "        Input:\n",
    "            S: (B, f, T) melspectrograms\n",
    "        Output:\n",
    "            Q: (B, d, T) queries\n",
    "        \"\"\"\n",
    "        super(AudioEnc, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            CausalConv(f, d, 1, 1, nonlinearity='relu'),\n",
    "            CausalConv(d, d, 1, 1, nonlinearity='relu'),\n",
    "            CausalConv(d, d, 1, 1),\n",
    "\n",
    "            CausalBasicBlock(d, 3, 1), CausalBasicBlock(d, 3, 3), CausalBasicBlock(d, 3, 9), CausalBasicBlock(d, 3, 27),\n",
    "            CausalBasicBlock(d, 3, 1), CausalBasicBlock(d, 3, 3), CausalBasicBlock(d, 3, 9), CausalBasicBlock(d, 3, 27),\n",
    "\n",
    "            CausalBasicBlock(d, 3, 3), CausalBasicBlock(d, 3, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class AudioDec(nn.Module):\n",
    "    def __init__(self, d=hp.d, f=hp.n_mels):\n",
    "        \"\"\"Audio decoder network.\n",
    "        Args:\n",
    "            d: Text2Mel dim\n",
    "            f: Number of mel bins\n",
    "        Input:\n",
    "            R_prime: (B, 2d, T) [V*Attention, Q] paper says: \"we found it beneficial in our pilot study.\"\n",
    "        Output:\n",
    "            Y: (B, f, T)\n",
    "        \"\"\"\n",
    "        super(AudioDec, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            CausalConv(2 * d, d, 1, 1),\n",
    "\n",
    "            CausalBasicBlock(d, 3, 1), CausalBasicBlock(d, 3, 3), CausalBasicBlock(d, 3, 9), CausalBasicBlock(d, 3, 27),\n",
    "\n",
    "            CausalBasicBlock(d, 3, 1), CausalBasicBlock(d, 3, 1),\n",
    "\n",
    "            # CausalConv(d, d, 1, 1, nonlinearity='relu'),\n",
    "            # CausalConv(d, d, 1, 1, nonlinearity='relu'),\n",
    "            CausalBasicBlock(d, 1, 1),\n",
    "            CausalConv(d, d, 1, 1, nonlinearity='relu'),\n",
    "\n",
    "            CausalConv(d, f, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Text2Mel(nn.Module):\n",
    "    def __init__(self, vocab, d=hp.d):\n",
    "        \"\"\"Text to melspectrogram network.\n",
    "        Args:\n",
    "            vocab: vocabulary\n",
    "            d: Text2Mel dim\n",
    "        Input:\n",
    "            L: (B, N) text inputs\n",
    "            S: (B, f, T) melspectrograms\n",
    "        Outputs:\n",
    "            Y_logit: logit of Y\n",
    "            Y: predicted melspectrograms\n",
    "            A: (B, N, T) attention matrix\n",
    "        \"\"\"\n",
    "        super(Text2Mel, self).__init__()\n",
    "        self.d = d\n",
    "        self.text_enc = TextEnc(vocab)\n",
    "        self.audio_enc = AudioEnc()\n",
    "        self.audio_dec = AudioDec()\n",
    "\n",
    "    def forward(self, L, S, monotonic_attention=False):\n",
    "        K, V = self.text_enc(L)\n",
    "        Q = self.audio_enc(S)\n",
    "        A = torch.bmm(K.permute(0, 2, 1), Q) / np.sqrt(self.d)\n",
    "\n",
    "        if monotonic_attention:\n",
    "            # TODO: vectorize instead of loops\n",
    "            B, N, T = A.size()\n",
    "            for i in range(B):\n",
    "                prva = -1  # previous attention\n",
    "                for t in range(T):\n",
    "                    _, n = torch.max(A[i, :, t], 0)\n",
    "                    if not (-1 <= n - prva <= 3):\n",
    "                        A[i, :, t] = -2 ** 20  # some small numbers\n",
    "                        A[i, min(N - 1, prva + 1), t] = 1\n",
    "                    _, prva = torch.max(A[i, :, t], 0)\n",
    "\n",
    "        A = F.softmax(A, dim=1)\n",
    "        R = torch.bmm(V, A)\n",
    "        R_prime = torch.cat((R, Q), 1)\n",
    "        Y_logit = self.audio_dec(R_prime)\n",
    "        Y = F.sigmoid(Y_logit)\n",
    "        return Y_logit, Y, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7f1806c-e1de-43a8-8881-67b8e84ed8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Wrapper class for logging into the TensorBoard and comet.ml\"\"\"\n",
    "__author__ = 'Erdene-Ochir Tuguldur'\n",
    "__all__ = ['Logger']\n",
    "\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "\n",
    "    def __init__(self, dataset_name, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.project_name = \"%s-%s\" % (dataset_name, self.model_name)\n",
    "        self.logdir = os.path.join(hp.logdir, self.project_name)\n",
    "        self.writer = SummaryWriter(log_dir=self.logdir)\n",
    "\n",
    "    def log_step(self, phase, step, loss_dict, image_dict):\n",
    "        if phase == 'train':\n",
    "            if step % 50 == 0:\n",
    "                # self.writer.add_scalar('lr', get_lr(), step)\n",
    "                # self.writer.add_scalar('%s-step/loss' % phase, loss, step)\n",
    "                for key in sorted(loss_dict):\n",
    "                    self.writer.add_scalar('%s-step/%s' % (phase, key), loss_dict[key], step)\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                for key in sorted(image_dict):\n",
    "                    self.writer.add_image('%s/%s' % (self.model_name, key), image_dict[key], step)\n",
    "\n",
    "    def log_epoch(self, phase, step, loss_dict):\n",
    "        for key in sorted(loss_dict):\n",
    "            self.writer.add_scalar('%s/%s' % (phase, key), loss_dict[key], step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45a13b20-953a-4c64-a2f9-c9c871097d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import *\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# project imports\n",
    "# from models import SSRN\n",
    "# from hparams import HParams as hp\n",
    "# from logger import Logger\n",
    "# from utils import get_last_checkpoint_file_name, load_checkpoint, save_checkpoint\n",
    "# from datasets.data_loader import SSRNDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb8ca9-65d1-47f8-93fc-3c11f559f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "text2mel = Text2Mel(vocab)\n",
    "#print(text2mel)\n",
    "text2mel.load_state_dict(torch.load(\"/content/drive/My Drive/TTSBn/logdir/ljspeech-text2mel/step-010K.pth\")['state_dict'])\n",
    "text2mel = text2mel.eval()\n",
    "ssrn = SSRN()\n",
    "ssrn.load_state_dict(torch.load(\"/content/drive/My Drive/TTSBn/logdir/ljspeech-ssrn/step-005K.pth\")['state_dict'])\n",
    "ssrn = ssrn.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
